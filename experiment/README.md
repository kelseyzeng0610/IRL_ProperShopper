# Overview of our experiment

1. Generate random shopping lists, each with 3 items. Choose some number `N` of shopping lists, say 50, and then run `python experiment.py --generate_lists --num_lists=50`. This will save a file `experiment/random_shopping_lists.json` containing the random lists, and it will also create a file `experiment/runs/run_i/start_state_i.txt` for each i in `1..N` which will contain a start state txt file. This allows us to pass that file in to the socket env so we can start the environment with that particular randomly generated shopping list.
2. Run expert demos on each random shopping list. Run the expert with `epsilon=0.2` 5 times to generate different trajectories. This is done with `python experiment.py --generate_irl_trajectories` and saves the trajectories to `experiment/runs/run_i/trajectories_i.pkl` for each i in `1..N`. 
3. Add noise to the expert demos. This is what we've been doing so far, so we do it again here. This loads each of the `trajectories_i.pkl` files and adds noise to each step, just as before. This is done with `python experiment.py --add_noise`. This will generate files `experiment/runs/run_i/noisy_trajectories_i.pkl` for i in `1..N`. 
4. Run our HIRL pipeline on each randomly generated shopping list. Read the noisy trajectories generated in the previous step, and feed those into our pipeline. This will segment into subgoals, and train a max entropy irl agent for each segment. Then it will save the learned agents to `experiment/runs/run_i/learned_agents_i.pkl` for i in `1..N`. This is done with `python experiment.py --run_irl`. This step is the longest step. I added parallelization, so we use a ProcessPoolExecutor to start up multiple workers, each worker running the training process independently on a single shopping list. This spawns separate python processes. Right now this is defaulting to 12 workers at a time, which is based on my machine having 12 cores. I recommend tuning this parameter to at most the number of cores your machine has, fewer is fine: `os.cpu_count()` in python will tell you this.
5. Now, we load the learned agents from the previous step and generate sample trajectories from their learned policies. For each random shopping list i in `1..N` we load `learned_agents_i.pkl`, then generate some number `M` of sample trajectories. We then write these to file at `experiment/runs/run_i/irl_generated_trajectory_i.json` (contains the trajectory as a list of states) and `experiment/runs/run_i/irl_generated_actions_i.json` which contains just the sequence of actions that produced that trajectory. This is done by running `python experiment.py --sample_irl_trajectories`.
6. Now, we run the generated sample trajectories by the IRL pipeline in the actual shopping environment. We load each of the `experiment/runs/run_i/irl_generated_actions_i.json` files, and run them in the socket, recording metrics (things like number of violations, number of steps, etc.) and saving the metrics to `experiment/runs/run_i/irl_generated_action_metrics_i.json` for i in `1..N`. This metrics file will be used later to gather our statistics about the different agents. This step is done by running `python experiment.py --run_hirl_samples`. 
7. We then run the expert once again, but without noise (setting `epsilon=0`). This represents our true expert behavior, the best possible set of actions the expert knows to take. This is only run a single time per shopping list, because it is deterministic. Metrics are once again collected and saved to `experiment/runs/run_i/metrics_expert_evaluation_i.json`.
8. TODO: this is not done yet. We need to run the baseline IRL agent on each of the shopping lists, generate sample trajectories, and run those in the environment to collect metrics and save that.
9. Now our data collection is complete. We then run a quick script to compute metrics on each of the different agents. This is done with `python experiment_results.py` which will create a json file `experiment/aggregated_results.json` containing some aggregate information of each of the models' performance across each shopping list. For each model (expert demos, expert with epsilon=0, IRL pipeline with segmentation, baseline, anything else), we record:
* `success_rate`: percent of runs that ended with all items collected and paid for
* `avg_violations_per_run`: the average number of violations committed by the agent in each run
* `percent_violation_free`: the percent of runs that had no violations
* `avg_num_steps`: the average number of steps taken by the agent to complete the task
* `avg_num_subgoals`: only applicable to the HIRL pipeline. the average number of subgoals it computed from the expert demos
* `avg_steps_between_subgoals`: only applicable to the HIRL pipeline. the average number of steps a subgoal took to complete.

